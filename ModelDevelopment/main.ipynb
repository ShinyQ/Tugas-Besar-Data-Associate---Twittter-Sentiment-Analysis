{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from typing import Dict,List,Tuple\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from gensim.models import Word2Vec\n",
    "import re as regex\n",
    "\n",
    "array = np.array\n",
    "tensor_array = tf.Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "BAKU = pd.read_csv(\"utils_info/kamus_alay.csv\")\n",
    "BAKU = BAKU.set_index(\"t_baku\")[\"baku\"].to_dict()\n",
    "def replace_at_to_User(text: str) -> str:\n",
    "    return regex.sub(\"@[a-zA-Z0-9]+\",\"user\",text)\n",
    "\n",
    "def remove_tanda(text : str) -> str:\n",
    "  text = regex.sub(\"[!\\\"#%$&\\'@()*+,-./:;<=>?[\\\\]^_`{|}~]+\",\"\",text)\n",
    "  return text\n",
    "\n",
    "def remove_links(text : str) -> str:\n",
    "    text = regex.sub(\"\\S*:\\S+\", \"\",text)\n",
    "    return text\n",
    "\n",
    "def modified_has_tag(text : str)-> str:\n",
    "    text =regex.sub(\"#\", \"kata kunci \", text).rstrip()\n",
    "    return text\n",
    "\n",
    "def stemming_data(text : str) -> str:\n",
    "    return MPStemmer().stem_kalimat(text)\n",
    "\n",
    "def map_to_baku(text, baku):\n",
    "    text_copy = text.split()\n",
    "    for i in range(len(text_copy)):\n",
    "        if text_copy[i] in baku:\n",
    "            text_copy[i] = baku[text_copy[i]]\n",
    "    \n",
    "    return \" \".join(text_copy)\n",
    "\n",
    "#%%\n",
    "def preprocessing_data(text):\n",
    "    text = text.lower();\n",
    "    text = map_to_baku(text,BAKU)\n",
    "    text = remove_links(text)\n",
    "    text = replace_at_to_User(text)\n",
    "    text = modified_has_tag(text)\n",
    "    text = remove_tanda(text)\n",
    "    #text = stemming_data(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(X : array , Y : array, max_features : int ,\n",
    "                 max_length : int, oov_token : str=\"<OOV>\",\n",
    "                 padding_type : str = \"post\" )->Tuple[object,tensor_array,array ]:\n",
    "    \n",
    "    Y  = Y.astype(int)\n",
    "    tokenizer = Tokenizer(num_words=max_features,oov_token=oov_token)\n",
    "    tokenizer.fit_on_texts(X)\n",
    "    x_sequences : tensor_array = tokenizer.texts_to_sequences(X)\n",
    "    x_padded = pad_sequences(x_sequences, maxlen=max_length, padding=padding_type)\n",
    "    Y = to_categorical(Y, num_classes=2)\n",
    "    return tokenizer, x_padded, Y\n",
    "\n",
    "def prepare_word_embeddings(raw_x : array, min_count : int, vector_size : int, window:int, sg:int,seed:int):\n",
    "    raw_x : List[str]= [text.split() for text in raw_x]\n",
    "    w2v_model : object = Word2Vec(raw_x, min_count=min_count, vector_size=vector_size, window=window, sg=sg, seed=seed)\n",
    "    w2v_model.save(f\"utils_info/gensim{vector_size}.model\")\n",
    "    return w2v_model\n",
    "    \n",
    "\n",
    "def createWordEmbeddings(w2v : object):\n",
    "    return pd.DataFrame(w2v[w2v.wv.vocab], index=list(w2v.wv.vocab))\n",
    "\n",
    "def createEmbeddingMatrix(tokenizer : object,w2v : object, max_features : int):\n",
    "    embedding_matrix : array = np.zeros((len(tokenizer.word_index) + 1, int(max_features)))\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        try:\n",
    "            embedding_vector = w2v.wv.get_vector(word)\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "        except:\n",
    "            embedding_matrix[i] = np.array([0] * max_features)\n",
    "    \n",
    "    return embedding_matrix\n",
    "\n",
    "def load_word_embeddings_model(path:str):\n",
    "    return Word2Vec.load(path)\n",
    "\n",
    "def prepare_embedding_layer(tokenizer : object, max_features:int, embedding_matrix : array, train : bool =False):\n",
    "    embedding_layer = Embedding(input_dim=len(tokenizer.word_index) + 1,\n",
    "    output_dim  = max_features,\n",
    "    weights = [embedding_matrix],\n",
    "    trainable  = train)\n",
    "    \n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#* just for naming types convetion\n",
    "array = np.array\n",
    "#%%\n",
    "train_data = pd.read_csv(\"./PreprocessedDataset/indolem/train.csv\")\n",
    "test_data = pd.read_csv(\"PreprocessedDataset/indolem/test.csv\")\n",
    "validation_data = pd.read_csv(\"PreprocessedDataset/indolem/validation.csv\")\n",
    "\n",
    "train_data_cleaned = train_data[\"sentence\"].apply(lambda x:preprocessing_data(x))  \n",
    "validation_data_cleaned = validation_data[\"sentence\"].apply(lambda x:preprocessing_data(x))\n",
    "\n",
    "X : array = train_data_cleaned.values\n",
    "Y : array = train_data[\"sentiment\"].values\n",
    "max_features :int= 10000\n",
    "max_length :int= 30\n",
    "\n",
    "#* Params For WordEmbeddings\n",
    "min_count : int = 3\n",
    "vector_size : int = max_length\n",
    "window : int = 5\t\n",
    "sg : int = 1\n",
    "seed : int = 0\n",
    "\n",
    "#* Preparing the data for the model\n",
    "\n",
    "#* Preparing WordEmbeddings For Models\n",
    "if not os.path.exists(\"./utils.info/*.model\"):\n",
    "\tw2v_model : object = prepare_word_embeddings(X, min_count, vector_size, window, sg, seed)\n",
    "else:\n",
    "\tpath : str = os.listdir(\"./utils.info/\")[0]\n",
    "\tw2v_model : object = load_word_embeddings_model(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        kangen nabil user user raga user user satu aca...\n",
       "1        doa untuk orang yang memberi makan iya allah b...\n",
       "2        setiap kali handphone saya bunyi saya selalu b...\n",
       "3        belum pernah sedekat ini wawancara dengan afga...\n",
       "4        dulu masa first pergi award show amatlah malas...\n",
       "                               ...                        \n",
       "18192    kamar 310 pintu kamar mandi nya tidak bisa di ...\n",
       "18193    tas hermes rp15 miliar dipakai belanja sayur o...\n",
       "18194                  deposit terlalu besar dan kamar bau\n",
       "18195    tanpa baju ruang angkasa darah seorang astrono...\n",
       "18196    cemburu itu tidak enak cemburu itu makan hati ...\n",
       "Name: sentence, Length: 18197, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0             pagi user user jumat berkah buat kita semua\n",
       "1       saya janji tidak akan pernah pergi dari dia se...\n",
       "2       kata kunci golkar kata kunci arb banyak member...\n",
       "3       tolak hamas langkah pemerintah dipuji partai d...\n",
       "4       sudirman masih calon kuat ketua dewan pimpinan...\n",
       "                              ...                        \n",
       "1990    ac tidak dingin lantai ktor pmesanan sebelum n...\n",
       "1991    seprei ada bekas noda televisi tidak berfungsi...\n",
       "1992    layanannya kurang pelayannya kurang cepat tanggap\n",
       "1993    ac nya kurang dingin di laman traveloka gambar...\n",
       "1994    tidak mampu beli xuser dua kantor pemerintah n...\n",
       "Name: sentence, Length: 1995, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_data_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 819, 2733,   16, ...,    0,    0,    0],\n",
       "       [1658,   17,   80, ...,    0,    0,    0],\n",
       "       [ 273,  130,   92, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [ 663,  143,  174, ...,    0,    0,    0],\n",
       "       [ 196,   79,  446, ...,    0,    0,    0],\n",
       "       [1434,   40,    2, ...,    0,    0,    0]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer,train_data_model,train_label_model = prepare_data(X, Y, max_features, max_length)\n",
    "train_data_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        , ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-0.77532756, -0.39504328, -0.03570122, ..., -0.11676707,\n",
       "         0.46571836,  0.44276375],\n",
       "       ...,\n",
       "       [-0.38438079,  0.05596161,  0.01910686, ..., -0.0320237 ,\n",
       "        -0.19091289,  0.32373103],\n",
       "       [-0.4099164 ,  0.10508002,  0.13727538, ...,  0.12667809,\n",
       "        -0.17868733,  0.43066597],\n",
       "       [-0.39311603,  0.06210523,  0.08185337, ...,  0.0710192 ,\n",
       "        -0.33428365,  0.46476793]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix : array = createEmbeddingMatrix(tokenizer,w2v_model, max_length)\n",
    "embedding_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.embeddings.Embedding at 0x1f557dceb38>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_layer = prepare_embedding_layer(tokenizer, max_length, embedding_matrix)\n",
    "embedding_layer"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "81e8d6ec8b128683c3ce30ee758ecb536232cfad698fab650870c6728bfc659d"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
